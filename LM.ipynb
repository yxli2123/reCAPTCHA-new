{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8ac089d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      loss 0.10645139217376709\n",
      "acc_single 0.9730933713471134\n",
      "  acc_pair 0.9465431218816821\n",
      "  acc_topk 0.9982181040627227\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import json\n",
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "\n",
    "with open(\"./code/results.json\", \"r\") as fp:\n",
    "    results = json.load(fp)\n",
    "    loss = results['loss']\n",
    "    acc_single = results['acc_single']\n",
    "    acc_pair = results['acc_pair']\n",
    "    acc_topk = results['acc_topk']\n",
    "    results = results['results']\n",
    "    \n",
    "print(\"      loss\", loss)\n",
    "print(\"acc_single\", acc_single)\n",
    "print(\"  acc_pair\", acc_pair)\n",
    "print(\"  acc_topk\", acc_topk)\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')\n",
    "model = BertForMaskedLM.from_pretrained('bert-base-chinese', return_dict = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d806761",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6b36a78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def prob(w1='中', w2='国'):\n",
    "    \n",
    "    w1_token = tokenizer(w1, add_special_tokens=False, return_tensors = \"pt\")['input_ids']\n",
    "    w2_token = tokenizer(w2, add_special_tokens=False, return_tensors = \"pt\")['input_ids']\n",
    "    #print(w1_token, w2_token)\n",
    "    \n",
    "    mask = tokenizer.mask_token\n",
    "    w1_given_w2 = tokenizer.encode_plus(mask+w2, return_tensors = \"pt\")\n",
    "    w2_given_w1 = tokenizer.encode_plus(w1+mask, return_tensors = \"pt\")\n",
    "    w1_given_w2 = {k: v.to('cuda:0') for k, v in w1_given_w2.items()}\n",
    "    w2_given_w1 = {k: v.to('cuda:0') for k, v in w2_given_w1.items()}\n",
    "    \n",
    "    mask_index_w1_given_w2 = torch.where(w1_given_w2[\"input_ids\"][0] == tokenizer.mask_token_id)\n",
    "    mask_index_w2_given_w1 = torch.where(w2_given_w1[\"input_ids\"][0] == tokenizer.mask_token_id)\n",
    "    \n",
    "    logits_w1_given_w2 = model(**w1_given_w2).logits[0, mask_index_w1_given_w2]\n",
    "    logits_w2_given_w1 = model(**w2_given_w1).logits[0, mask_index_w2_given_w1]\n",
    "    \n",
    "    p_w1_given_w2 = F.softmax(logits_w1_given_w2, dim=-1)\n",
    "    p_w2_given_w1 = F.softmax(logits_w2_given_w1, dim=-1)\n",
    "    #print(p_w1_given_w2.shape)\n",
    "    \n",
    "    #val_w1_given_w2, pos_w1_given_w2 = torch.topk(p_w1_given_w2, k=5)\n",
    "    #val_w2_given_w1, pos_w2_given_w1 = torch.topk(p_w2_given_w1, k=5)\n",
    "\n",
    "    #print(val_w1_given_w2, val_w2_given_w1)\n",
    "    #print(tokenizer.decode(pos_w1_given_w2[0]), tokenizer.decode(pos_w2_given_w1[0]))\n",
    "    \n",
    "    p_w1_given_w2 = p_w1_given_w2[0, w1_token]\n",
    "    p_w2_given_w1 = p_w2_given_w1[0, w2_token]\n",
    "    #print(p_w1_given_w2, p_w2_given_w1)\n",
    "    \n",
    "    return p_w1_given_w2, p_w2_given_w1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b6c9cd76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2806/2806 [29:07<00:00,  1.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5007127583749109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "ans_t = []\n",
    "for sample in tqdm(results):\n",
    "    prob_dict = {}\n",
    "    w1_candidates, w2_candidates = sample['char_pr'][0], sample['char_pr'][1]\n",
    "    \n",
    "    for w1, v1 in w1_candidates.items():\n",
    "        for w2, v2 in w2_candidates.items():\n",
    "            p_12, p_21 = prob(w1, w2)\n",
    "            p = 0.5 * (p_12 * v2 + p_21 * v1)\n",
    "            prob_dict[w1+w2] = p\n",
    "    \n",
    "    char_pr = max(prob_dict, key=prob_dict.get)\n",
    "    char_gt = sample['char_gt']\n",
    "    \n",
    "    if char_pr == char_gt:\n",
    "        ans_t.append(1)\n",
    "    else:\n",
    "        ans_t.append(0)\n",
    "\n",
    "acc = np.array(ans_t).mean()\n",
    "print(acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6798cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
